"""Backend for CPP run method"""
import os
import numpy as np
import pandas as pd
import multiprocessing as mp
from itertools import repeat
import warnings
from sklearn.metrics import roc_auc_score
from scipy import stats
from statsmodels.stats.multitest import multipletests

from ._split import SplitRange
import aaanalysis.utils as ut


# I Helper functions
def _get_splits(df_parts=None, split_type=None, split_type_args=None):
    """Split all given parts from df_parts. The split is given by its split_type
     (Segment, Pattern, or PeriodicPattern) and its arguments (split_kwargs)."""
    list_parts = list(df_parts)
    spr = SplitRange()
    f = getattr(spr, split_type.lower())
    if split_type_args is not None:
        f_splitr = lambda x: f(seq=x, **split_type_args)
        labels_s = getattr(spr, "labels_" + split_type.lower())(**split_type_args)
    else:
        f_splitr = lambda x: f(seq=x)   # Using default arguments
        labels_s = getattr(spr, "labels_" + split_type.lower())()
    # Combine part and split to get sequence splits
    part_splits = np.empty([len(df_parts), len(labels_s) * len(list_parts)], dtype=object)
    labels_ps = []  # Part Split labels
    for i, p in enumerate(list_parts):
        labels_ps.extend(["{}-{}".format(p.upper(), s) for s in labels_s])
        for j, seq in enumerate(df_parts[p]):
            part_splits[j, i*len(labels_s):(i+1)*len(labels_s)] = f_splitr(seq)
    return part_splits, labels_ps


def _p_correction(p_vals=None, p_cor="fdr_bh"):
    """Correct p-values"""
    # Exclude nan from list of corrected p-values
    p_vals_without_na = [p for p in p_vals if str(p) != "nan"]
    p_corrected_without_na = list(multipletests(p_vals_without_na, method=p_cor)[1])
    # Include nan in list of corrected p-values
    p_corrected = []
    i = 0
    for p in p_vals:
        if str(p) != "nan":
            p_corrected.append(p_corrected_without_na[i])
            i += 1
        else:
            p_corrected.append(np.nan)
    return p_corrected


# Pre-filtering and filtering
def _splitting(df_parts=None, split_kws=None):
    """Combine Part + Split

    Parameters
    ----------
    df_parts: :class:`pandas.DataFrame`
            DataFrame with sequence parts.
    split_kws: dict with kwargs for splitting

    Returns
    -------
    splittings: sequence fragments generated by applying splits on given sequence parts
    list_labels_ps: list with labels for Part-Split combination
    """
    list_parts_splits = []
    list_labels_ps = []
    for split_type in split_kws:
        split_type_args = split_kws[split_type]
        part_splits, labels_ps = _get_splits(df_parts=df_parts,
                                             split_type=split_type,
                                             split_type_args=split_type_args)
        list_parts_splits.append(part_splits)
        list_labels_ps.extend(labels_ps)
    splittings = np.concatenate(list_parts_splits, axis=1)
    return splittings, list_labels_ps

def _pre_filtering_info(list_scales, dict_all_scales, labels_ps, splittings, accept_gaps, mask_0, mask_1, verbose):
    """Compute abs(mean_dif) and std(test) to rank features, where mean_dif is the difference
    between the means of the test and the reference protein groups for a feature"""
    feat_names = np.empty((len(list_scales) * len(labels_ps)), dtype=object)
    abs_mean_dif = np.empty((len(list_scales) * len(labels_ps)))
    std_test = np.empty((len(list_scales) * len(labels_ps)))
    for i, scale in enumerate(list_scales):
        if verbose:
            ut.print_progress(i=i, n=len(list_scales))
        dict_scale = dict_all_scales[scale]
        start, end = i*len(labels_ps), (i+1)*len(labels_ps)
        # Feature names
        feat_names[start:end] = ["{}-{}".format(ps, scale) for ps in labels_ps]
        # Feature matrix
        vf_scale = ut.get_vf_scale(dict_scale=dict_scale, accept_gaps=accept_gaps)
        # TODO check missing values in ML
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)    # Filter numpy warning: "Mean of emtpy slice"
            X = np.round(vf_scale(splittings), 5)
        # Ranking infos
        abs_mean_dif[start:end] = abs(np.mean(X[mask_1], axis=0) - np.mean(X[mask_0], axis=0))
        std_test[start:end] = np.std(X[mask_1], axis=0)
    return abs_mean_dif, std_test, feat_names

def _filtering_info(df=None, df_scales=None, check_cat=True):
    """Get datasets structures for filtering, two dictionaries with feature to scale category resp.
    feature positions and one data sets frame with paired pearson correlations of all scales"""
    if check_cat:
        dict_c = dict(zip(df[ut.COL_FEATURE], df["category"]))
    else:
        dict_c = dict()
    dict_p = dict(zip(df[ut.COL_FEATURE], [set(x) for x in df["positions"]]))
    df_cor = df_scales.corr()
    return dict_c, dict_p, df_cor


# Summary and test statistics for feature matrix based on classification by labels
def _mean_dif(X=None, y=None):
    """ Get mean difference for values in X (feature matrix) based on y (labels)"""
    mask_0 = [True if x == 0 else False for x in y]
    mask_1 = [True if x == 1 else False for x in y]
    mean_difs = np.mean(X[mask_1], axis=0) - np.mean(X[mask_0], axis=0)
    return mean_difs


def _std(X=None, y=None, group=1):
    """Get standard deviation (std) for data sets points with group label"""
    if group not in y:
        raise ValueError("'group' must be in 'y'")
    mask = [True if x == group else False for x in y]
    group_std = np.std(X[mask], axis=0)
    return group_std


def _auc(X=None, y=None):
    """Get adjusted Area Under the Receiver Operating Characteristic Curve (ROC AUC)
    comparing, for each feature, groups (given by y (labels)) by feature values in X (feature matrix).
    """
    # Multiprocessing for AUC computation
    auc = np.apply_along_axis((lambda x: roc_auc_score(y, x) - 0.5), 0, X)
    auc = np.round(auc, 3)
    return auc


def _mean_stat(X=None, y=None, parametric=False, p_cor=None):
    """Statistical comparison of central tendency between two groups for each feature"""
    mask_0 = [True if x == 0 else False for x in y]
    mask_1 = [True if x == 1 else False for x in y]
    if parametric:
        p_vals = stats.ttest_ind(X[mask_1], X[mask_0], nan_policy="omit")[1]
        p_str = "p_val_ttest_indep"
    else:
        t = lambda x1, x2: stats.mannwhitneyu(x1, x2, alternative="two-sided")[1]  # Test statistic
        c = lambda x1, x2: np.mean(x1) != np.mean(x2) or np.std(x1) != np.std(x2)  # Test condition
        p_vals = np.round([t(col[mask_1], col[mask_0]) if c(col[mask_1], col[mask_0]) else 1 for col in X.T], 10)
        p_str = "p_val_mann_whitney"
    if p_cor is not None:
        p_vals = _p_correction(p_vals=p_vals, p_cor=p_cor)
        p_str = "p_val_" + p_cor
    return p_vals, p_str


# II Main functions
# Filtering methods
def pre_filtering_info(df_parts=None, split_kws=None, df_scales=None, y=None, accept_gaps=False, verbose=True,
                       n_processes=None):
    """Get n best features in descending order based on the abs(mean(group1) - mean(group0),
    where group 1 is the target group

    Parameters
    ----------
    df_parts: :class:`pandas.DataFrame`
        DataFrame with sequence parts.
    split_kws: dict, default = SequenceFeature.get_split_kws
        Nested dictionary with parameter dictionary for each chosen split_type.
    df_scales: :class:`pandas.DataFrame`, default = SequenceFeature.load_scales
        DataFrame with default amino acid scales.
    y: array-like, shape (n_samples)
        Class labels for samples in df_parts.
    accept_gaps: bool, default = False
        Whether to accept missing values by enabling omitting for computations (if True).
    verbose: bool, default = True
        Whether to print progress information about the algorithm (if True).
    n_processes: integer default = None
        Number of CPUs used for multiprocessing. If None, number will be optimized automatically

    Returns
    -------
    abs_mean_dif: array-like, shape (n_features)
        Absolute mean differences of feature values between samples of two groups.
    std_test: array-like, shape (n_features)
        Standard deviations of feature values in test group.
    feat_names: list of strings
        Names of all possible features for combination of Parts, Splits, and Scales.
    """
    # Input (df_parts, split_kws, df_scales, y) checked in main method (CPP.run())
    mask_0 = [True if x == 0 else False for x in y]
    mask_1 = [True if x == 1 else False for x in y]
    splittings, labels_ps = _splitting(split_kws=split_kws, df_parts=df_parts)
    list_scales = list(df_scales)
    dict_all_scales = {col: dict(zip(df_scales.index.to_list(), df_scales[col])) for col in list_scales}
    if n_processes is None or n_processes != 1:
        # Multiprocessing for filtering of features
        n_processes = min([os.cpu_count(), len(list_scales)])
        scale_chunks = np.array_split(list_scales, n_processes)
        args = zip(scale_chunks, repeat(dict_all_scales), repeat(labels_ps), repeat(splittings), repeat(accept_gaps),
                   repeat(mask_0), repeat(mask_1), repeat(verbose))
        with mp.get_context("spawn").Pool(processes=n_processes) as pool:
            result = pool.starmap(_pre_filtering_info, args)
        abs_mean_dif = np.concatenate([x[0] for x in result])
        std_test = np.concatenate([x[1] for x in result])
        feat_names = np.concatenate([x[2] for x in result])
    else:
        args = [list_scales, dict_all_scales, labels_ps, splittings, accept_gaps, mask_0, mask_1, verbose]
        abs_mean_dif, std_test, feat_names = _pre_filtering_info(*args)
    return abs_mean_dif, std_test, feat_names

def pre_filtering(features=None, abs_mean_dif=None, std_test=None, max_std_test=0.2, n=10000):
    """CPP pre-filtering based on thresholds."""
    df = pd.DataFrame(zip(features, abs_mean_dif, std_test),
                      columns=[ut.COL_FEATURE, ut.COL_ABS_MEAN_DIF, ut.COL_STD_TEST])
    df = df[df[ut.COL_STD_TEST] <= max_std_test]
    df = df.sort_values(by=ut.COL_ABS_MEAN_DIF, ascending=False).head(n)
    return df


def filtering(df=None, df_scales=None, max_overlap=0.5, max_cor=0.5, n_filter=100, check_cat=True):
    """CPP filtering algorithm based on redundancy reduction in descending order of absolute AUC."""
    dict_c, dict_p, df_cor = _filtering_info(df=df, df_scales=df_scales, check_cat=check_cat)
    df = df.sort_values(by=[ut.COL_ABS_AUC, ut.COL_ABS_MEAN_DIF], ascending=False).copy().reset_index(drop=True)
    list_feat = list(df[ut.COL_FEATURE])
    list_top_feat = [list_feat.pop(0)]  # List with best feature
    for feat in list_feat:
        add_flag = True
        # Stop condition for limit
        if len(list_top_feat) == n_filter:
            break
        # Compare features with all top features (added if low overlap & weak correlation or different category)
        for top_feat in list_top_feat:
            if not check_cat or dict_c[feat] == dict_c[top_feat]:
                # Remove if feat positions high overlap or subset
                pos, top_pos = dict_p[feat], dict_p[top_feat]
                overlap = len(top_pos.intersection(pos))/len(top_pos.union(pos))
                if overlap >= max_overlap or pos.issubset(top_pos):
                    # Remove if high pearson correlation
                    scale, top_scale = feat.split("-")[2], top_feat.split("-")[2]
                    cor = df_cor[top_scale][scale]
                    if cor > max_cor:
                        add_flag = False
        if add_flag:
            list_top_feat.append(feat)
    df_top_feat = df[df[ut.COL_FEATURE].isin(list_top_feat)]
    return df_top_feat

# Feature info methods
def add_stat(df=None, X=None, y=None, parametric=False):
    """Add summary statistics of feature matrix (X) for given labels (y) to df"""
    df = df.copy()
    columns_input = list(df)
    df[ut.COL_ABS_AUC] = abs(_auc(X=X, y=y))
    df[ut.COL_MEAN_DIF] = _mean_dif(X=X, y=y)
    if ut.COL_ABS_MEAN_DIF not in list(df):
        df[ut.COL_ABS_MEAN_DIF] = abs(_mean_dif(X=X, y=y))
    df[ut.COL_STD_TEST] = _std(X=X, y=y, group=1)
    df[ut.COL_STD_REF] = _std(X=X, y=y, group=0)
    p_val, p_str = _mean_stat(X=X, y=y, parametric=parametric)
    df[p_str] = p_val
    p_val_fdr, p_str_fdr = _mean_stat(X=X, y=y, parametric=parametric, p_cor="fdr_bh")
    df[p_str_fdr] = p_val_fdr
    cols_stat = [ut.COL_ABS_AUC,
                 ut.COL_ABS_MEAN_DIF, ut.COL_MEAN_DIF,
                 ut.COL_STD_TEST, ut.COL_STD_REF,
                 p_str, p_str_fdr]
    cols_in = [x for x in columns_input if x not in cols_stat and x != ut.COL_FEATURE]
    columns = [ut.COL_FEATURE] + cols_in + cols_stat
    df = df[columns]
    cols_round = [x for x in cols_stat if x not in [p_str, p_str_fdr]]
    df[cols_round] = df[cols_round].round(6)
    return df


