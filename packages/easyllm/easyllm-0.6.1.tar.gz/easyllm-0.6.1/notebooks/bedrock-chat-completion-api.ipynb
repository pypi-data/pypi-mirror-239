{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Chat Completion clients with Amazon Bedrock\n",
    "\n",
    "EasyLLM can be used as an abstract layer to replace `gpt-3.5-turbo` and `gpt-4` with Amazon Bedrock models.\n",
    "\n",
    "You can change your own applications from the OpenAI API, by simply changing the client. \n",
    "\n",
    "Chat models take a series of messages as input, and return an AI-written message as output.\n",
    "\n",
    "This guide illustrates the chat format with a few example API calls.\n",
    "\n",
    "## 0. Setup\n",
    "\n",
    "Before you can use `easyllm` with Amazon Bedrock you need setup permission and access to the models. You can do this by following of the instructions below:\n",
    "* https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-set-up.html\n",
    "* https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\n",
    "* https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the easyllm library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n",
    "%pip install --upgrade easyllm[bedrock] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the EasyLLM Python library for calling the EasyLLM API\n",
    "import easyllm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An example chat API call\n",
    "\n",
    "A chat API call has two required inputs:\n",
    "- `model`: the name of the model you want to use (e.g., `huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703`) or leave it empty to just call the api\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Compared to OpenAI api is the `huggingface` module also exposing a `prompt_builder` and `stop_sequences` parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with prompt builder utilities.\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': ' 2 + 2 = 4', 'stop_reason': 'stop_sequence'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'hf-Mf7UqliZQP',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1698333425,\n",
       " 'model': 'anthropic.claude-v2',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': '2 + 2 = 4'},\n",
       "   'finish_reason': 'stop_sequence'}],\n",
       " 'usage': {'prompt_tokens': 9, 'completion_tokens': 9, 'total_tokens': 18}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# set env for prompt builder\n",
    "os.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n",
    "\n",
    "from easyllm.clients import bedrock\n",
    "\n",
    "response = bedrock.ChatCompletion.create(\n",
    "    model=\"anthropic.claude-v2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "    ],\n",
    "      temperature=0.9,\n",
    "      top_p=0.6,\n",
    "      max_tokens=1024,\n",
    "      debug=False,\n",
    ")\n",
    "response\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
    "\n",
    "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': ' Okay class, today we\\'re going to learn about asynchronous programming. Asynchronous means things happening at different times, not necessarily in order. It\\'s like when you\\'re cooking dinner - you might put the pasta on to boil, then start chopping vegetables while the pasta cooks. You don\\'t have to wait for the pasta to finish boiling before you can start on the vegetables. The two tasks are happening asynchronously. \\n\\nIn programming, asynchronous functions allow the code to execute other operations while waiting for a long-running task to complete. Let\\'s look at an example:\\n\\n```js\\nfunction cookPasta() {\\n  console.log(\"Putting pasta on to boil...\");\\n  // Simulate a long task\\n  setTimeout(() => {\\n    console.log(\"Pasta done!\");\\n  }, 5000); \\n}\\n\\nfunction chopVegetables() {\\n  console.log(\"Chopping vegetables...\");\\n}\\n\\ncookPasta();\\nchopVegetables();\\n```\\n\\nWhen we call `cookPasta()`, it starts the timer but doesn\\'t wait 5 seconds - it immediately moves on to calling `chopVegetables()`. So the two functions run asynchronously. \\n\\nThe key is that `cookPasta()` is non-blocking - it doesn\\'t stop the rest of the code from running while it completes. This allows us to maximize efficiency and not waste time waiting.\\n\\nSo in summary, asynchronous programming allows multiple operations to happen independently of each other, like cooking a meal. We avoid blocking code execution by using asynchronous functions. Any questions on this?', 'stop_reason': 'stop_sequence'}\n",
      "Okay class, today we're going to learn about asynchronous programming. Asynchronous means things happening at different times, not necessarily in order. It's like when you're cooking dinner - you might put the pasta on to boil, then start chopping vegetables while the pasta cooks. You don't have to wait for the pasta to finish boiling before you can start on the vegetables. The two tasks are happening asynchronously. \n",
      "\n",
      "In programming, asynchronous functions allow the code to execute other operations while waiting for a long-running task to complete. Let's look at an example:\n",
      "\n",
      "```js\n",
      "function cookPasta() {\n",
      "  console.log(\"Putting pasta on to boil...\");\n",
      "  // Simulate a long task\n",
      "  setTimeout(() => {\n",
      "    console.log(\"Pasta done!\");\n",
      "  }, 5000); \n",
      "}\n",
      "\n",
      "function chopVegetables() {\n",
      "  console.log(\"Chopping vegetables...\");\n",
      "}\n",
      "\n",
      "cookPasta();\n",
      "chopVegetables();\n",
      "```\n",
      "\n",
      "When we call `cookPasta()`, it starts the timer but doesn't wait 5 seconds - it immediately moves on to calling `chopVegetables()`. So the two functions run asynchronously. \n",
      "\n",
      "The key is that `cookPasta()` is non-blocking - it doesn't stop the rest of the code from running while it completes. This allows us to maximize efficiency and not waste time waiting.\n",
      "\n",
      "So in summary, asynchronous programming allows multiple operations to happen independently of each other, like cooking a meal. We avoid blocking code execution by using asynchronous functions. Any questions on this?\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = bedrock.ChatCompletion.create(\n",
    "    model=\"anthropic.claude-v2\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': \" Aye matey! Asynchronous programming be when ye fire yer cannons without waiting fer each shot to hit. Ye keep loadin' and shootin' while the cannonballs sail through the air. Ye don't know exactly when they'll strike the target, but ye keep sendin' 'em off. \\n\\nThe ship keeps movin' forward, not stalled waiting fer each blast. Other pirates keep swabbin' the decks and hoistin' the sails so we make progress while the cannons thunder. We tie callbacks to the cannons to handle the boom when they finally hit.\\n\\nArrr! Asynchronous programmin' means ye do lots o' tasks at once, not blocked by waitin' fer each one to finish. Ye move ahead and let functions handle the results when ready. It be faster than linear code that stops at each step. Thar be treasures ahead, lads! Keep those cannons roarin'!\", 'stop_reason': 'stop_sequence'}\n",
      "Aye matey! Asynchronous programming be when ye fire yer cannons without waiting fer each shot to hit. Ye keep loadin' and shootin' while the cannonballs sail through the air. Ye don't know exactly when they'll strike the target, but ye keep sendin' 'em off. \n",
      "\n",
      "The ship keeps movin' forward, not stalled waiting fer each blast. Other pirates keep swabbin' the decks and hoistin' the sails so we make progress while the cannons thunder. We tie callbacks to the cannons to handle the boom when they finally hit.\n",
      "\n",
      "Arrr! Asynchronous programmin' means ye do lots o' tasks at once, not blocked by waitin' fer each one to finish. Ye move ahead and let functions handle the results when ready. It be faster than linear code that stops at each step. Thar be treasures ahead, lads! Keep those cannons roarin'!\n"
     ]
    }
   ],
   "source": [
    "# example without a system message and debug flag on:\n",
    "response = bedrock.ChatCompletion.create(\n",
    "    model=\"anthropic.claude-v2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': \" Changing direction at the last minute means we don't have time to do an exhaustive analysis for what we're providing to the client.\", 'stop_reason': 'stop_sequence'}\n",
      "Changing direction at the last minute means we don't have time to do an exhaustive analysis for what we're providing to the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = bedrock.ChatCompletion.create(\n",
    "    model=\"anthropic.claude-v2\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every attempt at engineering conversations will succeed at first.\n",
    "\n",
    "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
    "\n",
    "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
    "\n",
    "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
