{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Inference Endpoints Example\n",
    "\n",
    "**[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/)** offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security.\n",
    "\n",
    "You can get started with Inference Endpoints at: https://ui.endpoints.huggingface.co/\n",
    "\n",
    "\n",
    "The example assumes that you have an running endpoint for a conversational model, e.g. `https://huggingface.co/meta-llama/Llama-2-13b-chat-hf`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the easyllm library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade easyllm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An example chat API call\n",
    "\n",
    "Since we want to use our endpoint for inference we don't have to define the `model` parameter. We either need to expose an environment variable `HUGGINGFACE_API_BASE` before the import of `easyllm.clients.huggingface` or overwrite the `huggingface.api_base` value.\n",
    "\n",
    "A chat API call then only has two required inputs:\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'hf-0lL5H_yyRR',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1691096023,\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': ' Apple who?'},\n",
       "   'finish_reason': 'eos_token'}],\n",
       " 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easyllm.clients import huggingface\n",
    "\n",
    "# Here we overwrite the defaults, you can also use environment variables\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "huggingface.api_base = \"YOUR_ENDPOINT_URL\"\n",
    "\n",
    "# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n",
    "# huggingface.api_key=\"hf_xxx\"\n",
    "\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Apple.\"},\n",
    "    ],\n",
    "      temperature=0.9,\n",
    "      top_p=0.6,\n",
    "      max_tokens=1024,\n",
    ")\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apple who?\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to stream Chat Completion requests\n",
    "\n",
    "Custom endpoints can be created to stream chat completion requests to a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here we go:\n",
      "\n",
      "1. One\n",
      "2. Two\n",
      "3. Three\n",
      "4. Four\n",
      "5. Five\n",
      "6. Six\n",
      "7. Seven\n",
      "8. Eight\n",
      "9. Nine\n",
      "10. Ten!"
     ]
    }
   ],
   "source": [
    "from easyllm.clients import huggingface\n",
    "\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "\n",
    "# Here you can overwrite the url to your endpoint, can also be localhost:8000\n",
    "huggingface.api_base = \"YOUR_ENDPOINT_URL\"\n",
    "\n",
    "# a ChatCompletion request\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"Count to 10.\"}\n",
    "    ],\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if \"content\" in delta:\n",
    "        print(delta[\"content\"],end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
