{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using Llama 2\n",
    "\n",
    "This notebook walks through how to use Llama 2 to perform (in-context) retrieval augmented generation. We will customize the `system` message for Llama 2 to make sure the model is only using provided context to generate the response. \n",
    "\n",
    "**What is In-context Retrieval Augmented Generation?**\n",
    "\n",
    "\n",
    "In-context retrieval augmented generation is a method to improve language model generation by including relevant documents to the model input. The key points are:\n",
    "\n",
    "* Retrieval of relevant documents from an external corpus to provide factual grounding for the model.\n",
    "* Prepending the retrieved documents to the input text, without modifying the model architecture or fine-tuning the model.\n",
    "* Allows leveraging external knowledge with off-the-shelf frozen language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n",
    "%pip install --upgrade easyllm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "\n",
    "Below is a simple example using the existing prompt builder of llama2 to generate a prompt. We are going to use the `system` message from [llama-index](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemoLlama-Local.html) with some minor adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given #SOURCE# documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "- Only include facts and information based on the #SOURCE# documents.\n",
    "\"\"\"\n",
    "\n",
    "system = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we can now call our LLM. Lets create a user instruction with a `query` and a `context`. As a context i copied the the [wikipedia article of Nuremberg](https://en.wikipedia.org/wiki/Nuremberg) (the city i live). \n",
    "_I uploaded it as a gist to to not pollute the notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://gist.githubusercontent.com/philschmid/2678351cb9f41d385aa5c099caf20c0a/raw/60ae425677dd9bed6fe3c0f2dd5b6ea49bc6590c/nuremberg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = open(\"nuremberg.txt\").read()\n",
    "\n",
    "query = \"How many people live in Nuremberg?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use our context lets just ask the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As of December 31, 2020, the population of Nuremberg, Germany is approximately 516,000 people.\n"
     ]
    }
   ],
   "source": [
    "from easyllm.clients import huggingface\n",
    "\n",
    "# set the prompt builder to llama2\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "# huggingface.api_key = \"hf_xx\"\n",
    "\n",
    "# send a ChatCompletion request\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# print the time delay and text received\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use our `system` message with our `context` to augment the knowledge of our model \"in-memory\" and ask the same question again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_extended = f\"{query}\\n\\n#SOURCE#\\n{context}\"\n",
    "# context_extended = f\"{query}\\n\\n#SOURCE START#\\n{context}\\n#SOURCE END#{query}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The population of Nuremberg is 523,026 according to the 2022-12-31 data.\n"
     ]
    }
   ],
   "source": [
    "from easyllm.clients import huggingface\n",
    "\n",
    "# set the prompt builder to llama2\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "# huggingface.api_key = \"hf_xx\"\n",
    "\n",
    "# send a ChatCompletion request\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[\n",
    "        system, \n",
    "        {\"role\": \"user\", \"content\": context_extended},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# print the time delay and text received\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! if we check the gist, we can see a snippet in there with saying\n",
    "```bash\n",
    "Population (2022-12-31)[2]\n",
    " â€¢ City\t523,026\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Next steps, would be to connect your LLM using with external knowledge sources such as Wikis, the Web or other databases using tools and APIs or vector databases and embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
