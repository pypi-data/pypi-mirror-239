{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to stream Chat Completion requests\n",
    "\n",
    "By default, when you request a completion, the entire completion is generated before being sent back in a single response.\n",
    "\n",
    "If you're generating long completions, waiting for the response can take many seconds.\n",
    "\n",
    "To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.\n",
    "\n",
    "To stream completions, set `stream=True` when calling the chat completions or completions endpoints. This will return an object that streams back the response as [data-only server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). Extract chunks from the `delta` field rather than the `message` field.\n",
    "\n",
    "## Downsides\n",
    "\n",
    "Note that using `stream=True` in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate. \n",
    "## Example code\n",
    "\n",
    "Below, this notebook shows:\n",
    "1. What a typical chat completion response looks like\n",
    "2. What a streaming chat completion response looks like\n",
    "3. How much time is saved by streaming a chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import easyllm  # for API calls\n",
    "import time  # for measuring time duration of API calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What a typical chat completion response looks like\n",
    "\n",
    "With a typical ChatCompletions API call, the response is first computed and then returned all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response received 0.12 seconds after request\n",
      "Full response received:\n",
      "{'id': 'hf-JhxbFCGVUW', 'object': 'chat.completion', 'created': 1691129826, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}, 'finish_reason': 'eos_token'}], 'usage': {'prompt_tokens': 25, 'completion_tokens': 400, 'total_tokens': 425}}\n"
     ]
    }
   ],
   "source": [
    "from easyllm.clients import huggingface\n",
    "\n",
    "# set the prompt builder to llama2\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a ChatCompletion request to count to 100\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# calculate the time it took to receive the response\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
    "print(f\"Full response received:\\n{response}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reply can be extracted with `response['choices'][0]['message']`.\n",
    "\n",
    "The content of the reply can be extracted with `response['choices'][0]['message']['content']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted reply: \n",
      "{'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}\n",
      "Extracted content: \n",
      " Sure! Here it is:\n",
      "\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n"
     ]
    }
   ],
   "source": [
    "reply = response['choices'][0]['message']\n",
    "print(f\"Extracted reply: \\n{reply}\")\n",
    "\n",
    "reply_content = response['choices'][0]['message']['content']\n",
    "print(f\"Extracted content: \\n{reply_content}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How to stream a chat completion\n",
    "\n",
    "With a streaming API call, the response is sent back incrementally in chunks via an [event stream](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). In Python, you can iterate over these events with a `for` loop.\n",
    "\n",
    "Let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': 'assistant'}}]}\n",
      "{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'content': ' '}}]}\n",
      "{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'content': ' Two'}}]}\n",
      "{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {}}]}\n"
     ]
    }
   ],
   "source": [
    "from easyllm.clients import huggingface\n",
    "\n",
    "\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "\n",
    "# a ChatCompletion request\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n",
    "    ],\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, streaming responses have a `delta` field rather than a `message` field. `delta` can hold things like:\n",
    "- a role token (e.g., `{\"role\": \"assistant\"}`)\n",
    "- a content token (e.g., `{\"content\": \"\\n\\n\"}`)\n",
    "- nothing (e.g., `{}`), when the stream is over"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How much time is saved by streaming a chat completion\n",
    "\n",
    "Now let's ask `meta-llama/Llama-2-70b-chat-hf` to count to 100 again, and see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message received 0.13 seconds after request: {'role': 'assistant'}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': ' Sure'}\n",
      "Message received 0.13 seconds after request: {'content': '!'}\n",
      "Message received 0.13 seconds after request: {'content': ' Here'}\n",
      "Message received 0.13 seconds after request: {'content': ' it'}\n",
      "Message received 0.13 seconds after request: {'content': ' is'}\n",
      "Message received 0.13 seconds after request: {'content': ':'}\n",
      "Message received 0.13 seconds after request: {'content': '\\n'}\n",
      "Message received 0.13 seconds after request: {'content': '\\n'}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '5'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '6'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '7'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '8'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '9'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '0'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '5'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '6'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '7'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '8'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': '9'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '0'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '5'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '6'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '7'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '8'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': '9'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '0'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '5'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '6'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '7'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '8'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': '9'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '0'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '1'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '2'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '3'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '5'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '6'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '7'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '8'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '4'}\n",
      "Message received 0.13 seconds after request: {'content': '9'}\n",
      "Message received 0.13 seconds after request: {'content': ','}\n",
      "Message received 0.13 seconds after request: {'content': ' '}\n",
      "Message received 0.13 seconds after request: {'content': '5'}\n",
      "Message received 0.13 seconds after request: {'content': '0'}\n",
      "Message received 0.13 seconds after request: {}\n",
      "Full response received 0.13 seconds after request\n",
      "Full conversation received:   Sure! Here it is:\n",
      "\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from easyllm.clients import huggingface\n",
    "\n",
    "huggingface.prompt_builder = \"llama2\"\n",
    "\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a ChatCompletion request to count to 100\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "    ],\n",
    "    stream=True  # again, we set stream=True\n",
    ")\n",
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
    "    collected_messages.append(chunk_message)  # save the message\n",
    "    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
    "print(f\"Full conversation received: {full_reply_content}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time comparison\n",
    "\n",
    "In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.\n",
    "\n",
    "However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
