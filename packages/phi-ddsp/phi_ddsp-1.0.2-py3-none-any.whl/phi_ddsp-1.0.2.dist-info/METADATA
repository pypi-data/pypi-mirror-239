Metadata-Version: 2.1
Name: phi-ddsp
Version: 1.0.2
Summary: phi
Author-email: Max Ardito <maxwellardito@gmail.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Classifier: License :: OSI Approved :: MIT License
Requires-Dist: numpy>=1.19.4
Requires-Dist: crepe>=0.0.11
Requires-Dist: librosa>=0.8.0
Requires-Dist: einops>=0.3.0
Requires-Dist: tqdm>=4.46.0
Requires-Dist: torch>=1.7.0
Requires-Dist: effortless_config>=0.7.0
Requires-Dist: SoundFile>=0.10.3.post1
Requires-Dist: Flask>=1.1.2
Requires-Dist: PyYAML>=5.3.1
Requires-Dist: tensorboard
Requires-Dist: pytest
Requires-Dist: datasets
Project-URL: home-page, https://github.com/maxardito/phi

# Φ

A conditional timbral modeling tool based off of the concept of Differentiable Digital
Signal Processing

## Φαντασμαγορία

**_Today is the shadow of tomorrow, today is the present future of yesterday, yesterday is
the shadow of today, the darkness of the past is yesterday_**

--- Madlib, Quasimoto

An homage to Brian Kane's Sound Unseen, and the need for more historically informed work
concerning generative modeling.

## Usage

Edit the `config.yaml` file to fit your needs (audio location, preprocess folder, sampling rate, model parameters...), then preprocess your data using

```bash
python preprocess.py
```

You can then train your model using

```bash
python train.py --name mytraining --steps 10000000 --batch 16 --lr .001
```

Each flag is an override of the configuration provided in `config.yaml`.

You can monitor the progress with tensorboard

```bash
tensorboard --logdir models/train
```

Once trained, export it using

```bash
python export.py --run models/mytraining
```

It will produce a file named `ddsp_pretrained_mytraining.ts`, that you can use inside a python environment like that

```python
import torch

model = torch.jit.load("ddsp_pretrained_mytraining.ts")

pitch = torch.randn(1, 200, 1)
loudness = torch.randn(1, 200, 1)

audio = model(pitch, loudness)
```

