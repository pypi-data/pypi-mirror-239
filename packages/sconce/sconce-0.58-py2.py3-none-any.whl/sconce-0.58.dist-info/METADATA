Metadata-Version: 2.1
Name: sconce
Version: 0.58
Summary: sconce: torch pipeliner  
Home-page: https://github.com/satabios/sconce
Author: Sathyaprakash Narayanan
Author-email: Sathyaprakash Narayanan <snaray17@ucsc.edu>
License: MIT License
Project-URL: code, https://github.com/satabios/sconce
Keywords: sconce
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Mathematics
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: certifi >=2023.5.7
Requires-Dist: charset-normalizer >=3.2.0
Requires-Dist: cmake >=3.27.0
Requires-Dist: contourpy >=1.1.0
Requires-Dist: cycler >=0.11.0
Requires-Dist: filelock >=3.12.2
Requires-Dist: fonttools >=4.41.0
Requires-Dist: idna >=3.4
Requires-Dist: Jinja2 >=3.1.2
Requires-Dist: kiwisolver >=1.4.4
Requires-Dist: lit >=16.0.6
Requires-Dist: MarkupSafe >=2.1.3
Requires-Dist: matplotlib >=3.7.2
Requires-Dist: mpmath >=1.3.0
Requires-Dist: networkx >=3.1
Requires-Dist: numpy >=1.24.1
Requires-Dist: packaging >=23.1
Requires-Dist: Pillow >=10.0.0
Requires-Dist: pyparsing >=3.0.9
Requires-Dist: python-dateutil >=2.8.2
Requires-Dist: requests >=2.31.0
Requires-Dist: six >=1.16.0
Requires-Dist: sympy >=1.12
Requires-Dist: torch
Requires-Dist: snntorch
Requires-Dist: torchprofile >=0.0.4
Requires-Dist: torchvision >=0.15.2
Requires-Dist: tqdm >=4.65.0
Requires-Dist: typing-extensions >=4.7.1
Requires-Dist: urllib3 >=2.0.4
Requires-Dist: transformers >=0.1.0
Requires-Dist: fast-pytorch-kmeans
Requires-Dist: ipdb

====================================
sconce (Model Compression Made Easy)
====================================


.. image:: https://readthedocs.org/projects/sconce/badge/?version=latest
        :target: https://sconce.readthedocs.io/en/latest/?badge=latest
        :alt: Documentation Status

.. image:: https://github.com/satabios/sconce/blob/master/docs/source/images/sconce-punch-bk_removed.png?raw=true
        :align: center
        :width: 400

.. raw:: html

   </p>

This is a Pytorch Helper package aimed to aid the workflow of deep
learning model development, compression and deployment.

- This packages has boiler plate defintions that can ease the development of torch model development
- This package has a set of compression techniques that can be used to compress the model

NOTE:
      * Pruning Techniques are imported from Tomoco Package(deperecated)
      * Model Quantization and Deployment features are in the development pipeline which will be available for use soon. 
         
Package install:
================
.. code:: python

   pip install sconce



Example:
========

Define Network and Config’s:
----------------------------

.. code:: python

   # Define your network

   class Net(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(3, 8, 3)
           self.bn1 = nn.BatchNorm2d(8)
           self.pool = nn.MaxPool2d(2, 2)
           self.conv2 = nn.Conv2d(8, 16, 3)
           self.bn2 = nn.BatchNorm2d(16)
           self.fc1 = nn.Linear(16*6*6, 32)
           self.fc2 = nn.Linear(32, 10)

       def forward(self, x):
           x = self.pool(self.bn1(F.relu(self.conv1(x))))
           x = self.pool(self.bn2(F.relu(self.conv2(x))))
           x = torch.flatten(x, 1)
           x = F.relu(self.fc1(x))
           x = self.fc2(x)
           return x

Make a Dict for Dataloader
--------------------------

.. code:: python


   image_size = 32
   transforms = {
       "train": Compose([
           RandomCrop(image_size, padding=4),
           RandomHorizontalFlip(),
           ToTensor(),
       ]),
       "test": ToTensor(),
   }
   dataset = {}
   for split in ["train", "test"]:
     dataset[split] = CIFAR10(
       root="data/cifar10",
       train=(split == "train"),
       download=True,
       transform=transforms[split],
     )
   dataloader = {}
   for split in ['train', 'test']:
     dataloader[split] = DataLoader(
       dataset[split],
       batch_size=512,
       shuffle=(split == 'train'),
       num_workers=0,
       pin_memory=True,
     )

Define your Configurations:
---------------------------

.. code:: python

   # Define all parameters 

   from sconce import sconce

   sconces = sconce()
   sconces.model= Net() # Model Definition
   sconces.criterion = nn.CrossEntropyLoss() # Loss
   sconces.optimizer= optim.Adam(sconces.model.parameters(), lr=1e-4)
   sconces.scheduler = optim.lr_scheduler.CosineAnnealingLR(sconces.optimizer, T_max=200)
   sconces.dataloader = dataloader
   sconces.epochs = 5 #Number of time we iterate over the data
   sconces.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   sconces.experiment_name = "vgg-gmp" # Define your experiment name here
   sconces.prune_mode = "GMP" # Prune Mode: Currently supporting "GMP"(Supports Automated Pruning Ratio Detection), "CWP". Future supports for "OBC" and "sparseGPT"



One Roof Solution [Train -> Compress -> Deploy]:
------------------------------------------------

.. code:: python


   sconces.compress()

To-Do
~~~~~

-  ☒ Universal Channel-Wise Pruning

-  ☒ Update Tutorials

-  ☒ Fine Grained Purning

-  ☒ Channel Wise Purning

-  [+] OBC Compression (In-Progress)

-  [+] Spasegpt like Pruning (In-Progress)

-  [+] Quantisation (In-Progress)

-  ☐ Universal AutoML package

-  ☐ Introduction of Sparsification in Pipeline
